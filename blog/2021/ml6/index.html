<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Mixture Models and EM | Aofan Jiang</title> <meta name="author" content="Aofan Jiang"> <meta name="description" content="Book Notes on Pattern Recognition and Machine Learning (PRML)"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <script src="https://code.iconify.design/iconify-icon/1.0.7/iconify-icon.min.js"></script> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.png"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jjjaaafff.github.io/blog/2021/ml6/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Mixture Models and EM",
      "description": "Book Notes on Pattern Recognition and Machine Learning (PRML)",
      "published": "August 5, 2021",
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span>Aofan Jiang (蒋傲凡)</span></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Mixture Models and EM</h1> <p>Book Notes on Pattern Recognition and Machine Learning (PRML)</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#k-means-clustering">K-means Clustering</a></div> <div><a href="#expectation-maximization-algorithm">Expectation Maximization Algorithm</a></div> <div><a href="#em-for-gaussian-mixture">EM for Gaussian Mixture</a></div> <div><a href="#connection-between-k-means-and-em">Connection between K-means and EM</a></div> </nav> </d-contents> <h2 id="k-means-clustering">K-means Clustering</h2> <p>K-means 聚类是一种无监督学习，从数据本身的特征出发，不依靠标签将数据划分为不同类别。K-means 的算法流程很直观，因此我们先介绍 K-means 聚类的实现流程，再从数学角度刻画分析</p> <p>K-means 算法流程如下：</p> <ol> <li>首先选择 K 值，即最终将数据聚为 K 类。</li> <li>初始化，在每个类中选择一个数据点将其划分进去，作为该类的中心</li> <li>对每个数据点，将其划分到距类中心最近的类中</li> <li>所有点划分完成后，根据划分好的数据更新这 K 个类中心点的位置</li> <li>重新分配每一个数据点到最近的类中</li> <li>重复步骤 4 和 5直到收敛</li> </ol> <p>在介绍完算法流程后，我们用数学形式去刻画该方法。我们假设数据集为 \(\{x_1,...,x_n\}\) ，需要将其聚为 k 类。每个类别 k 的中心点记为 \(\mu_k\) ，由此我们可以写出距离公式作为损失函数，即</p> \[L = \sum_{i=1}^n\sum_{j=1}^k\delta_{ij}||x_i-\mu_j||^2\] <p>这里的 \(\delta_{ij}\) 为 0-1 函数，如果数据 i 被划分到类别 j，那么 \(\delta_{ij}=1\) ；否则为零。我们的目标就是找到每个数据的划分方式 \(\delta\) ，以及各聚类的中心点 \(\mu\) ，使得loss最小。我们可以通过迭代的方式来实现目标。我们最开始先确定初始的中心点 \(\mu\)</p> <p>接下来第一步是确定划分方式 \(\delta\) 。此时我们已经有上一步得到的各聚类中心点 \(\mu\) ，那么对于某个数据 \(x_i\) 而言，我们写出划分方式对应的更新公式</p> \[\delta_{ik} = 1 \quad if\ k=argmin_{j}||x_i-\mu_j||^2 \qquad (else\ \ 0)\] <p>在迭代的第二步，我们要修改中心点的位置。我们对损失函数求导，可以得到其关于 \(\mu\) 的导数</p> \[\frac{\partial L}{\partial \mu_j} = 2\sum_{i=1}^n\delta_{ij}(\mu_j-x_i)=0 \Rightarrow \mu_j = \frac{\sum_{i}\delta_{ij}x_i}{\sum_i\delta_{ij}}\] <p>此外，我们也可以利用在线的随机算法来实现聚类中心 \(\mu\) 的更新。对某个数据 \(x_i\) ，我们更新距其最近的聚类中心 \(\mu_j\) 的值，即</p> \[\mu_j^{new} = \mu_{j}^{old} + \eta(x_i-\mu_{j}^{old})\] <p>最后是关于 K-means 算法的一些细节</p> <ul> <li>收敛的标准是数据点不再移动到其它类，同时各类的中心点位置不变。此时对应的Loss可能只是极小值，不一定是全局最小（这与初始聚类中心的选取有关）</li> <li>算法的复杂度：每一次迭代是 O(KN) ，K 是类别数，N 是数据总数</li> <li>聚类数 K 的选取：尝试不同的 K ，计算数据点到类中心的平均距离。该距离会急剧下降直到合适的 K 值，然后缓慢变化</li> </ul> <h2 id="expectation-maximization-algorithm">Expectation Maximization Algorithm</h2> <p>EM 算法是用来寻找含隐变量的概率模型极大似然解的一种方法。和上面一样，我们还是先给出 EM 算法的流程，再从数学角度刻画分析。</p> <p>我们记观察变量为 X，隐变量为 Z，模型参数统记为 \(\theta\) 。EM 算法用来求解后验概率 \(p(X\vert\theta)\) 的极大似然，包含 E 和 M 两步。</p> <ul> <li>E-step: 计算 \(Q(\theta\vert\theta^{(t)})\) ，这里</li> </ul> \[Q(\theta\vert\theta^{(t)}) = E_{Z\sim P(Z\vert X,\theta^{(t)})}[\ln P(X,Z\vert \theta)]\] <ul> <li>M-step: 选择使 \(Q(\theta\vert\theta^{(t)})\) 最大的参数，用来更新当前模型参数，即</li> </ul> \[\theta^{(t+1)} = argmax_{\theta}Q(\theta\vert\theta^{(t)})\] <p>可以看到，这是一个迭代的过程，因此整体的 EM 算法流程可写为</p> <ol> <li>初始化，给出初始的模型参数 \(\theta^{(0)}\)</li> <li>E-step，在给定模型 \(\theta\) 下，计算不同隐变量值 Z 的概率，进而计算 \(Q(\theta\vert\theta^{(t)})\)</li> <li>M-step，利用计算的 Q 值更好地修改模型</li> <li>重复 2 和 3 步骤直至收敛</li> </ol> <p>在介绍完 EM 算法的流程后，很自然我们有两个问题。E-step 中的优化目标 \(Q(\theta\vert\theta^{(t)})\) 是从何而来的呢？EM算法为什么是正确的呢？</p> <p>我们首先来看第一个问题，E-step中的优化目标从何而来？对此我们有两种解释方式</p> <ul> <li>首先我们想用极大似然估计，用观测数据去确定模型参数，即</li> </ul> \[max_{\theta}\ln P(X\vert \theta) = max_{\theta} \ln \sum_{Z}P(X,Z\vert \theta)\] <p>这里 Z 作为未知的隐变量，表示 X 中各个数据所属的类别。由于 Z 未知，我们无法直接得到原优化目标里的 \(P(X,Z\vert \theta)\) 。但如果给出 X 和 \(\theta\) ，我们可以求出 Z 对应的后验分布，即 \(P(Z\vert X,\theta)\) 。所以我们先根据现有模型去猜测 Z 的分布，再用这个猜测的 Z 去计算极大似然，从而改善模型参数。这样我们就得到了 E-step中的优化目标，即 \(max_{\theta} E_{Z\sim P(Z\vert X,\theta^{(t)})}[\ln P(X,Z\vert \theta)]\)</p> <ul> <li>第二种解释方式是梯度更新的角度。我们的原目标为极大似然 \(max_{\theta} \ln P(X\vert\theta)\) ，我们对优化目标求导可以得到</li> </ul> \[\Delta\theta = \eta\frac{1}{P(X\vert\theta)}\frac{\partial}{\partial\theta}P(X\vert\theta) = \eta\frac{1}{P(X\vert\theta)}\sum_{Z}\frac{\partial}{\partial\theta}P(X,Z\vert\theta)\] \[\Delta\theta = \eta\frac{1}{P(X\vert\theta)}\sum_{Z}P(X,Z\vert\theta)\frac{\partial}{\partial\theta}\ln P(X,Z\vert\theta)\] \[\Delta\theta = \eta\sum_{Z}P(Z\vert X,\theta)\frac{\partial}{\partial\theta}\ln P(X,Z\vert\theta)\] <p>在这里我们进行近似，用 \(\theta^{(t)}\) 代替上式中的 \(\theta\) ，就可以得到</p> \[\Delta\theta = \eta\sum_{Z}P(Z\vert X,\theta^{(t)})\frac{\partial}{\partial\theta}\ln P(X,Z\vert\theta)\] <p>也就是E-step中的优化目标</p> \[\Delta\theta = \eta\frac{\partial}{\partial\theta} E_{Z\sim P(Z\vert X,\theta^{(t)})}[\ln P(X,Z\vert \theta)]\] <p>在解释了 E-step 中优化目标的来源后，下一个问题是为什么 EM 算法是正确的。对此，我们同样有两种证明方式</p> <ul> <li>第一种证明用到了 Jensen 不等式，即对于凸函数 \(f(\cdot)\) 而言，我们有 \(E[f(x)] \ge f(E[x])\) ，而</li> </ul> \[\ln P(X\vert \theta) = \ln \sum_{Z}P(X,Z\vert \theta) = \ln \sum_{Z}P(Z\vert X,\theta^{(t-1)})\frac{P(X,Z\vert \theta)}{P(Z\vert X,\theta^{(t-1)})}\] <p>其中 \(\sum_{Z}P(Z\vert X,\theta^{(t-1)}) = 1\) ，再结合 Jensen 不等式，我们就可以得到</p> \[\ln P(X\vert \theta) \ge \sum_{Z}P(Z\vert X,\theta^{(t-1)}) [\ln P(X,Z\vert \theta) - \ln P(Z\vert X,\theta^{(t-1)})]\] <p>即</p> \[\ln P(X\vert \theta) \ge Q(\theta\vert\theta^{(t)}) + constant\] <p>这意味着我们在优化 \(Q(\theta\vert\theta^{(t)})\) 的过程中，实际上是在不断提升原目标的下界，所以相当于也在提升原目标。</p> <ul> <li>第二种证明利用了条件概率，即</li> </ul> \[\forall Z, \qquad \log P(X\vert \theta) = \log P(X,Z\vert\theta) - \log P(Z\vert X,\theta)\] <p>我们改写原目标可以得到</p> \[\ln P(X\vert \theta) = [\sum_{Z'}P(Z'\vert X,\theta^{(t)})]\ln P(X\vert \theta)\] <p>既然条件概率中的 Z 可以是任意值，那么我们就令 Z 的值为 Z’，代入上式可以得到</p> \[\ln P(X\vert \theta) = [\sum_{Z'}P(Z'\vert X,\theta^{(t)})][\log P(X,Z'\vert\theta) - \log P(Z'\vert X,\theta)] = Q(\theta\vert\theta^{(t)})+H(\theta\vert\theta^{(t)})\] <p>这里的 \(H(\cdot)\) 类似与信息熵的定义，我们有 \(H(\theta\vert\theta^{(t)}) \ge H(\theta^{(t)}\vert\theta^{(t)})\) ，因此我们就可以得到</p> \[\ln P(X\vert\theta)-\ln P(X\vert\theta^{(t)}) \geq Q(\theta\vert\theta^{(t)}) - Q(\theta^{(t)}\vert\theta^{(t)})\] <p>同样说明我们是在不断提升原目标的下界。</p> <h2 id="em-for-gaussian-mixture">EM for Gaussian Mixture</h2> <p>高斯混合分布可以写成不同高斯分布的线性叠加，即</p> \[p(x) = \sum_{i=1}^K \pi_iN(x\vert \mu_i,\Sigma_i)\] <p>接下来我们引入隐变量 z，z 是 K 维的0-1向量，向量中只有一个元素为1，其余元素为0，即 \(\sum_{i=1}^Kz_i=1\) 。为了得到和上式相似的形式，我们用混合系数 \(\pi_i\) 来刻画隐变量 z 的分布，可以写出</p> \[p(z_i=1)=\pi_i, \quad 0\le \pi_i\le 1, \quad \sum_{i=1}^K\pi_i=1\] <p>现在我们就可以写出隐变量 z 的边缘分布 \(p(z)\)</p> \[p(z) = \prod_{i=1}^K \pi_i^{z_i}\] <p>当我们给出一个确定的 z 时，也就是说我们知道了 z 的具体某个元素 \(z_i\) 为一，此时 x 的条件分布同样服从高斯，\(x\vert z_i=1 \sim N(\mu_i,\Sigma_i)\)</p> <p>我们写出整体的条件分布</p> \[p(x\vert z) = \prod_{i=1}^KN(x\vert \mu_i,\Sigma_i)^{z_i}\] <p>在同时得到边缘分布和条件分布后，我们可以写出 x 和 z 的联合分布，进而求出 x 的分布</p> \[p(x) = \sum_zp(z)p(x\vert z) = \sum_z\prod_{i=1}^K (\pi_iN(x|\mu_i,\Sigma_i))^{z_i} = \sum_{i=1}^K \pi_iN(x\vert \mu_i,\Sigma_i)\] <p>在引入隐变量 z 后，我们得到了和原高斯分布等价的形式。我们引入隐变量的目的就是将目标从 x 的边缘分布 \(p(x)\) 转换到 x 和 z 的联合分布 \(p(x,z)\) 。</p> <p>在正式介绍 EM 在高斯混合模型的应用之前，我们还需要引入一个概念，隐变量 z 的后验分布 \(p(z\vert x)\) 。我们用 \(\gamma(z_k)\) 来表示 \(p(z_k=1\vert x)\) ，根据贝叶斯定理可以写出</p> \[\gamma(z_k) = \frac{p(z_k=1)p(x\vert z_k=1)}{\sum_{i=1}^Kp(z_i=1)p(x\vert z_i=1)} = \frac{\pi_kN(x\vert \mu_k,\Sigma_k)}{\sum_{i=1}^K\pi_iN(x\vert \mu_i,\Sigma_i)}\] <p>假设我们有数据集 \(\{x_1,...,x_n\}\) ，我们想用高斯混合模型进行建模，那么此时的模型包含三个参数 \(\pi, \mu, \Sigma\) 。考虑用极大似然来优化模型，我们有</p> \[\ln p(x\vert \pi, \mu, \Sigma) = \sum_{i=1}^n\ln\{\sum_{j=1}^K \pi_jN(x_i\vert \mu_j,\Sigma_j)\}\] <p>首先将其对 \(\mu_k\) 求导并令其为零，可以得到</p> \[\sum_{i=1}^n\gamma(z_{ik})\Sigma^{-1}_k(x_i-\mu_k) = 0 \rightarrow \mu_k = \frac{\sum_{i=1}^n\gamma(z_{ik})x_i}{\sum_{i=1}^n\gamma(z_{ik})}\] <p>其次将其对 \(\Sigma_k\) 求导并令其为零，可以得到</p> \[\Sigma_k = \frac{\sum_{i=1}^n\gamma(z_{ik})(x_i-\mu_k)(x_i-\mu_k)^T}{\sum_{i=1}^n\gamma(z_{ik})}\] <p>最后我们需要考虑 \(\sum_{i=1}^K\pi_i=1\) 的限制条件，利用拉格朗日乘子法将问题转化为</p> \[\ln p(x\vert \pi, \mu, \Sigma)+ \lambda(\sum_{i=1}^K\pi_i-1)\] <p>对 \(\pi_k\) 求导并令其为零，得到 \(\pi_k\) 的计算公式</p> \[\lambda + \sum_{i=1}^n\frac{N(x_i\vert \mu_k,\Sigma_k)}{\sum_{j=1}^K\pi_jN(x_i\vert \mu_j,\Sigma_j)}=0 \rightarrow \pi_k = \frac{\sum_{i=1}^n\gamma(z_{ik})}{n}\] <p>最后我们总结下 EM 算法在高斯混合模型的应用</p> <ul> <li>初始化，确定模型初始值 \(\pi_k, \mu_k, \Sigma_k\)</li> <li>E-step: 根据现有模型值，计算 \(\gamma\)</li> </ul> \[\gamma(z_{nk}) = \frac{\pi_kN(x_n\vert \mu_k,\Sigma_k)}{\sum_{i=1}^K\pi_iN(x_n\vert \mu_i,\Sigma_i)}\] <ul> <li>M-step: 根据计算的 \(\gamma\) 值重新估计更新模型参数</li> </ul> \[\mu_k = \frac{\sum_{i=1}^n\gamma(z_{ik})x_i}{\sum_{i=1}^n\gamma(z_{ik})}\] \[\Sigma_k = \frac{\sum_{i=1}^n\gamma(z_{ik})(x_i-\mu_k)(x_i-\mu_k)^T}{\sum_{i=1}^n\gamma(z_{ik})}\] \[\pi_k = \frac{\sum_{i=1}^n\gamma(z_{ik})}{n}\] <ul> <li>收敛之后，我们就可以去估计 log-likelihood</li> </ul> \[\ln p(x\vert \pi, \mu, \Sigma) = \sum_{i=1}^n\ln\{\sum_{j=1}^K \pi_jN(x_i\vert \mu_j,\Sigma_j)\}\] <p>可以发现，在上述介绍中，并没有明显体现出隐变量 z 的作用。在这里我们从隐变量的角度重新看待 EM 算法。在引入隐变量后，我们可以写出联合分布的极大似然</p> \[p(x,z\vert\pi, \mu,\Sigma) = \prod_{i=1}^n\prod_{j=1}^K\{\pi_jN(x_i\vert \mu_j,\Sigma_j)\}^{z_{ij}}\] <p>其对应的 log-likelihood 为</p> \[\ln p(x,z\vert\pi, \mu,\Sigma) = \sum_{i=1}^n\sum_{j=1}^K z_{ij}\{\ln \pi_j + \ln N(x_i\vert \mu_j,\Sigma_j)\}\] <p>和原问题中的极大似然 \(\ln p(x\vert \pi, \mu, \Sigma)\) 相比，最显著的特点是我们将求和操作从 log 里面拿到了外面，这可以大大简化问题的求解。因为我们引入了隐变量，最后还需要通过期望消除，所以最后的优化目标为</p> \[E_z[\ln p(x,z\vert\pi, \mu,\Sigma)] = \sum_{i=1}^n\sum_{j=1}^K \gamma(z_{ij})\{\ln \pi_j + \ln N(x_i\vert \mu_j,\Sigma_j)\}\] <p>该优化目标下参数 \(\pi, \mu, \Sigma\) 的更新方式和上文介绍的一模一样。</p> <h2 id="connection-between-k-means-and-em">Connection between K-means and EM</h2> <p>与 K-means 算法相比，EM 算法达到收敛需要迭代更多次数，并且每次迭代也需要更多的计算量。所以通常的做法是先用 K-means 来确定模型的初始化参数，再用 EM 算法改进提升。</p> <p>实际上，K-means 算法可以看成是一种特殊极限情况下的 EM 算法结合高斯混合模型。</p> <p>我们假设所有数据的协方差矩阵由共享的常数 \(\epsilon\) 控制，即 \(\Sigma = \epsilon I\) ，此时 x 在第 i 个高斯模型下的分布可以写出来</p> \[p(x\vert \mu_i,\Sigma_i) = \frac{1}{(2\pi\epsilon)^{1/2}}exp\{-\frac{1}{2\epsilon}||x-\mu_i||^2\}\] <p>进一步写出后验概率 \(\gamma(z_{nk})\)</p> \[\gamma(z_{nk}) = \frac{\pi_kexp\{-||x_n-\mu_k||^2/2\epsilon\}}{\sum_{i=1}^K\pi_iexp\{-||x_n-\mu_i||^2/2\epsilon\}}\] <p>当 \(\epsilon\) 逐渐减小直至趋近于零时，\(\vert\vert x_n-\mu_i\vert\vert^2\) 最小的那一项最晚降为零，因此只有这一项为一，其余都很快地变为零。即此时 \(\gamma(z_{nk}) \rightarrow \delta_{nk}\) ，每个数据点都被唯一地划分到均值最小的对应聚类中。此时 M-step 中均值的更新退化为</p> \[\mu_k = \frac{\sum_{i=1}^n\gamma(z_{ik})x_i}{\sum_{i=1}^n\gamma(z_{ik})} \rightarrow \mu_k = \frac{\sum_{i}\delta_{ik}x_i}{\sum_i\delta_{ik}}\] <p>再写出 \(\epsilon \rightarrow 0\) 下的log-likelihood 的期望</p> \[E_z[\ln p(x,z\vert\pi, \mu,\Sigma)] = -\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^K \delta_{ij}||x_i-\mu_j||^2 + constant\] <p>此时最大化上式中的log-likelihood等价于最小化k-means算法中的loss</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2024 Aofan Jiang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: February 27, 2024. </div> <div id="map-container" style="display: none;"> <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&amp;w=300&amp;t=n&amp;d=vcetvMKxQeU0A74GGVddvtKdDYpzY562Hjs3OBdOjBw"></script> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>