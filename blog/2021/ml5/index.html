<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Support Vector Machine | Aofan Jiang</title> <meta name="author" content="Aofan Jiang"> <meta name="description" content="Book Notes on Pattern Recognition and Machine Learning (PRML)"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <script src="https://code.iconify.design/iconify-icon/1.0.7/iconify-icon.min.js"></script> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.png"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jjjaaafff.github.io/blog/2021/ml5/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Support Vector Machine",
      "description": "Book Notes on Pattern Recognition and Machine Learning (PRML)",
      "published": "July 31, 2021",
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span>Aofan Jiang (蒋傲凡)</span></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Support Vector Machine</h1> <p>Book Notes on Pattern Recognition and Machine Learning (PRML)</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#svm-with-outlier">SVM with outlier</a></div> <div><a href="#kernel-svm">Kernel SVM</a></div> <div><a href="#relevance-vector-machine-rvm">Relevance Vector Machine （RVM）</a></div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>在之前的线性模型中，有时我们可以找到很多个模型都可以正确地进行分类。那么此时我们就需要选一个“最好”的模型。也就是说，我们不仅希望模型能正确分类，还希望它能尽可能地把不同类数据分得更开。</p> <p>假设是 \(\{+1,-1\}\) 的二分类问题。为了衡量模型这种分类的信心，我们定义</p> \[r_i = y_i(w^Tx_i+b)\] <p>此时，\(r_i\) 的值越大，表示模型对数据 \(x_i\) 的分类信心越大。然而，我们发现如果同时扩大 w 和 b 相同的倍数，会导致 \(r_i\) 同样变大，但分类结果不会有改变。所以我们还需要归一化，修改后的定义为</p> \[r_i = \frac{y_i(w^Tx_i+b)}{||w||_2}\] <p>这里的 \(r_i\) 可以理解为数据点 i 到分类平面的距离。我们关注的是分类信心最小的那些点，即需要提升 \(r\) 的下界。（在教材中，这些最小的分类信心被称为margin，优化问题为最大化margin），可以写出对应的数学形式</p> \[max_{w,b}(min_{i=1,...,n} (r_i))\] <p>改写一下，用 \(\tau=min(r_i)\) 来表示margin，此时的优化目标为</p> \[max_{\tau,w,b} \tau \qquad s.t.\ \forall i\ \ y_i(w^Tx_i+b)\geq\tau, \quad ||w||=1\] <p>为了简便表示，将约束条件中的归一化挪到优化目标里，我们得到</p> \[max_{\tau,w,b} \frac{\tau}{||w||} \qquad s.t.\ \forall i\ \ y_i(w^Tx_i+b)\geq\tau\] <p>我们可以发现，此时margin的具体值对最终得到的解没有影响，所以我们设为1，得到最终的 SVM 优化问题</p> \[min_{w,b} \frac{1}{2}||w||^2 \qquad s.t.\ \forall i\ \ y_i(w^Tx_i+b)\geq 1\] <p>我们之所以加上 1/2 的系数是为了后续求解的方便。现在我们已经得到了 SVM 对应的优化问题，下一步就是求解。</p> <p>因为原问题很难求解，我们使用拉格朗日乘子法将其转化为对偶问题求解。这里首先介绍拉格朗日乘子法。我们假设原优化问题为</p> \[min_w f(w) \quad s.t.\ g_k(w)\leq 0, k=1,...,K;\ h_{l}(w)=0,l=1,...,L\] <p>对应的对偶问题为</p> \[L(w,\alpha,\beta) =f(w)+\sum_{i=1}^K\alpha_ig_i(w)+\sum_{i=1}^L\beta_ih_i(w)\] <p>此时的约束条件也被称为 KKT condition，即</p> \[\forall k\ \alpha_kg_k(w) = 0, \quad \forall k \ g_k(w)\leq 0, \quad \forall k\ \alpha_k\geq 0\] <p>回到 SVM 的求解中，我们现在的优化问题为</p> \[min_{w,b} \frac{1}{2}||w||^2 \qquad s.t.\ \forall i\ \ 1-y_i(w^Tx_i+b)\leq 0\] <p>我们写出对偶问题</p> \[min_{w,b} max_{\alpha:\alpha\geq 0} L(w,b,\alpha)\] \[where\quad L(w,b,\alpha)=\frac{1}{2}||w||^2 + \sum_{i=1}^n \alpha_i[1-y_i(w^Tx_i+b)]\] <p>我们分别对 w 和 b 求偏导，以消除 L 中的这两个变量只剩下 \(\alpha\)，可以得到</p> \[\frac{\partial L}{\partial w} = 0 \Rightarrow w = \sum_{i=1}^n\alpha_iy_ix_i\] \[\frac{\partial L}{\partial b} = 0 \Rightarrow \sum_{i=1}^n\alpha_iy_i = 0\] <p>将这两个式子代入到 \(L(w,b,\alpha)\) 中消除变量后，我们可以得到只关于 \(\alpha\) 的优化目标，即</p> \[max_{\alpha} L(\alpha) = \sum_{i=1}^n\alpha_i - \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^ny_iy_j\alpha_i\alpha_jx_i^Tx_j\] \[s.t. \quad \forall i \ \alpha_i\geq 0, \sum_{i=1}^n\alpha_iy_i = 0\] <p>这个优化问题就可以更容易地求解，最常用的一种求解方式称为 sequential minimal optimization（SMO）。在求解得到 \(\alpha\) 后，我们代回到 w 和 b 的式子中，可以得到</p> \[w = \sum_{i:\alpha_i &gt; 0}\alpha_iy_ix_i\] \[b = \frac{1}{|S|}\sum_{s\in S}(y_s-\sum_{i\in S}\alpha_iy_ix_i^Tx_s),\quad where\ S = \{j\vert \alpha_j &gt;0\}\] <p>自此我们就求解得到了模型（w 和 b），接下来我们对这个结果进行一下分析。<br> 正如之前提到的，我们在使用拉格朗日乘子转换为对偶问题时，需要满足 KKT 条件，即</p> \[\forall i,\quad \alpha_i \geq 0,\quad y_i(w^Tx_i+b)-1\geq 0,\quad a_i(1-y_i(w^Tx_i+b))=0\] <p>这意味着，对于每一个数据 \(x_i\) ，要么对应的 \(\alpha_i=0\) ，要么 \(y_i(w^Tx_i+b)=1\) 。前者表明该数据很容易被分类，对最后的模型没有影响。后者表明该数据处于离分类平面最近的位置，是最难分类的。所有满足后者的数据构成了 Support Vector 支撑向量。我们也只用到了支持向量有关的数据作为模型的一部分。</p> <p>所以当模型训练结束后，大部分训练数据都可以丢弃，只保留构成支持向量的数据。这些数据对应最难分类的一系列样本，正是这样一些数据把单个分类平面支撑起来，所以被称为支持向量机</p> <h2 id="svm-with-outlier">SVM with outlier</h2> <p>在之前的介绍中，我们假设训练数据是完全可分类的，而这是一种很强的假设。在这一节，我们通过引入误差项，允许一些样本的分类信心没有那么强，允许一些样本被分错类。我们记误差项为 \(\xi\) ，正则化系数为常数 \(C\) ，此时的优化问题可写为</p> \[min_{\xi,w,b}\frac{1}{2}||w||^2 + C\sum_{i=1}^n\xi_i\quad s.t. \forall i, \ \ y_i(w^Tx_i+b)\geq 1-\xi_i,\ \xi_i\geq 0\] <p>等价于</p> \[min_{\xi,w,b}\frac{1}{2}||w||^2 + C\sum_{i=1}^n max(0,1-y_i(w^Tx_i+b))\] <p>和上面一样的变化方法，这里直接写出对偶问题中的优化函数</p> \[L(w,b,\xi,\alpha,\beta) = \frac{1}{2}||w||^2 + C\sum_{i=1}^n\xi_i + \sum_{i=1}^n\alpha_i[1-\xi_i-y_i(w^Tx_i+b)] - \sum_{i=1}^n \beta_i\xi_i\] <p>和上面的流程一样，接着我们对 w b 和 \(\xi\) 求偏导消除变量，w 和 b 的偏导结果和标准SVM一样，对 \(\xi\) 求偏导则可得到</p> \[\frac{\partial L}{\partial \xi_i} = 0 \Rightarrow \alpha_i + \beta_i = C\] <p>代入原函数后，我们得到变换后的优化问题</p> \[max_{\alpha} L(\alpha) = \sum_{i=1}^n\alpha_i - \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^ny_iy_j\alpha_i\alpha_jx_i^Tx_j\] \[s.t. \quad \forall i \ 0\leq \alpha_i \leq C, \sum_{i=1}^n\alpha_iy_i = 0\] <p>最后我们对结果进行分析。这里因为引入了误差项 \(\xi\) 不等式，KKT条件变为</p> \[\forall i,\quad \alpha_i \geq 0,\quad y_i(w^Tx_i+b)-1+\xi_i\geq 0,\quad a_i(y_i(w^Tx_i+b)-1+\xi_i)=0\] \[\forall i,\quad \beta_i \geq 0,\quad \xi_i\geq 0,\quad \beta_i\xi_i=0\] <ul> <li>如果 \(\alpha_i =0\)，那么此时 \(\beta_i = C-\alpha_i &gt;0, \xi_i=0\) 这个数据 i 很容易被分类，落在margin外面，其误差项为零，对最终训练好的模型没有影响</li> <li>如果 \(0&lt;\alpha_i &lt;C\)，同样有\(\beta_i = C-\alpha_i &gt;0, \xi_i=0\)，此时该数据落在margin上，参与构成了support vector</li> <li>如果 \(\alpha_i =C\)，那么此时 \(\beta_i = C-\alpha_i =0, \xi_i&gt;0\) 此时误差项不为零了，说明该数据落在 margin 内部。 <ul> <li>如果 \(\xi_i&lt;1\) ，说明该数据仍可以被正确分类，但分类的信心很弱，落在分类平面和margin之间。</li> <li>如果 \(\xi_1&gt;1\) ，说明该数据被彻底地错误分类了，落在分类平面的另一边。我们可以发现，尽管如此，其正则项保证该数据对模型的影响随着数据的偏离成正比</li> </ul> </li> </ul> <p>可以看出，这样增加误差项的处理方式防止了过拟合，提升了对训练数据的泛化能力</p> <h2 id="kernel-svm">Kernel SVM</h2> <p>对于非线性的分类问题，有时需要用核函数 \(\phi(X)\) 代替 X 。此时的SVM可表示为 \(Y = w^T\phi(X)+b\)</p> <p>求解可以得到 \(w=\sum_{i=1}^n\alpha_iy_i^*\phi(x_i)\) ，代入原模型可得</p> \[y_j = (\sum_{i=1}^n\alpha_iy_i^*\phi(x_i))^T\phi(x_j)+b = \sum_{i:\alpha&gt;0} \alpha_iy_i^*k(x_i,x_j) + b\] <p>我们可以发现，SVM并不能提供概率输出，即每个数据属于某个类的概率。为了解决这个问题，其中一种方式再拟合，即</p> \[p(y^*=1\vert x) = \sigma(Ay(x)+B)\] <p>这里 \(\sigma(\cdot)\) 是logistic sigmoid函数，\(y(x)\) 为SVM的输出值，A和B为通过交叉熵loss学习的参数。我们需要另一套数据集来训练这个 logistic 模型，必须与之前训练 SVM 的数据集相互独立，否则会有严重的过拟合现象。即使这样，效果仍然很差</p> <p>最后我们来看下 SVM 和 Logistic regression 的联系。<br> 首先是 SVM，在软间隔的SVM中，优化目标为</p> \[min_{\xi,w,b}\frac{1}{2}||w||^2 + C\sum_{i=1}^n max(0,1-y_i(w^Tx_i+b))\] <p>把它写成Loss的形式可以得到</p> \[L = \sum_{i=1}^n max(0,1-y_i^*y_i) + \lambda ||w||^2,\quad where\ \ \lambda = \frac{1}{2C}\] <p>我们再来看 logistics regression，之前我们针对的是 \(\{0,1\}\) 标签。为了保持和 SVM 标签一致，我们假设为 \(\{+1,-1\}\) 标签，则有</p> \[p(y^*_i=1\vert x) = \frac{1}{1+e^{-x_i^T\beta}}, \quad p(y^*_i=-1\vert x)= 1- p(1) = \frac{1}{1+e^{x_i^T\beta}}\] <p>因此可以得到通式</p> \[p(y_i^*\vert y_i) = \frac{1}{1+e^{-y_i^*y_i}}\] \[L = \sum_{i=1}^n \ln(1+e^{-y_i^*y_i}) + \lambda ||w||^2\] <p>可以看出两者 Loss 函数的相似与不同。</p> <h2 id="relevance-vector-machine-rvm">Relevance Vector Machine （RVM）</h2> <p>之前的SVM存在一些缺陷，比如它直接输出分类决策而不是后验概率，主要用来处理二分类问题等。因此又提出了RVM，在保留SVM特点的同时避免了SVM的局限性。</p> <p>RVM类似与之前的贝叶斯线性回归，给定输入数据 x 后，ground-truth 的分布为 \(y^*\vert x,w \sim N(y(x),\sigma^2)\) ，这里的 \(y(x)\) 写成基函数的形式，即</p> \[y(x) = \sum_{i=1}^{n+1} w_i\phi_i(x) = w^T\phi(x)\] <p>我们用矩阵表示，写出似然函数</p> \[p(Y^*\vert X,w) = \prod_{i=1}^np(y^*_i\vert x_i,w) \Rightarrow Y^*\vert X,w \sim N(\phi w,\sigma^2 I)\] <p>这里 \(\phi\) 是一个 n*(n+1) 的矩阵，\(\phi_{ij} = \phi_j({x_i})\) 。和之前不同的一点是，RVM关键区别是为每个权参数 \(w_i\) 都引入一个单独的超参数 \(\alpha_i\) ，而不是像之前一样共享一个超参数 \(\Sigma\) ，所以此时的模型先验分布为</p> \[p(w\vert \alpha) = \prod_{i=1}^{n+1}N(0,\alpha_i) \Rightarrow w\vert \alpha \sim N(0,\alpha)\] <p>至此结合以上表达式，我们可以写出模型的后验概率 \(p(w\vert X,Y)\) ，即</p> \[w\vert X,Y \sim N(\mu,\Sigma)\] \[where \quad \mu = \sigma^{-2}\Sigma \phi^TY, \quad \Sigma = ((\alpha I)^{-1}+\sigma^{-2}\phi^{T}\phi)^{-1}\] <p>如果我们再知道超参 \(\alpha\) 和 \(\sigma\) 的值，就可以对新数据进行预测了。所以接下来的一步是求这两个超参。我们通过对权向量积分最大化边缘似然函数，即</p> \[p(Y^*\vert X) = \int p(Y^*\vert X,w)p(w\vert\alpha)dw = \int N(Y^*\vert \phi w,\sigma^2I)N(w\vert 0,\alpha)dw\] <p>令上式导数为零，可以得到估计方程</p> \[\alpha_i^{new} = \frac{1-\alpha_i \Sigma_{ii}}{\mu_i^2},\qquad (\sigma^2)^{new} = \frac{||Y^*-\phi\mu||^2}{n-\sum_i(1-\alpha_i \Sigma_{ii})}\] <p>所以超参 \(\alpha\) 和 \(\sigma\) 的整个学习流程为</p> <ol> <li>选择 \(\alpha\) 和 \(\sigma\) 的初始值，利用上面的后验概率公式均值和方差求解公式计算后验概率均值 \(\mu\) 和方差 \(\Sigma\)</li> <li>利用前文中 \(\alpha_i^{new}\) 和 \((\sigma^2)^{new}\) 的计算公式交替重新估计超参数。</li> <li>重新估计后验均值和方差，以此类推，直到满足合适的收敛准则。</li> </ol> <p>我们观察收敛后的超参，可以发现一部分 \(\alpha_i\) 趋于无穷大。那么对应的模型权值 \(w_i\) 的均值和方差都趋近零，也就意味着这些权值及对应的基函数对模型没有贡献。这就和之前的SVM相似了，剩下的非零权值输入 \(x_n\) 称为相关向量，类似与SVM中的支持向量。</p> <p>我们用 \(\alpha^\star\) 和 \(\sigma^\star\) 来表示最后学到的超参。在预测阶段，我们假设输入数据为 \(x_0\)，那么此时可以写出预测输出的概率分布</p> \[p(y_{pred}\vert x_0) = \int p(y_{pred}\vert x_0,w)p(w\vert X,Y)dw = \int N(y_{pred}\vert \phi(x_0)^Tw,(\sigma^*)^2)N(w\vert\mu,\Sigma)dw\] \[y_{pred}\vert x_0 \sim N(\mu^T\phi(x_0),(\sigma^*)^2+\phi(x_0)^T\Sigma\phi(x_0))\] <p>我们就可以根据上式得到我们最后的预测输出 \(y_{pred}\)</p> <p>以上是 RVM 在回归任务中的应用，接下来我们简要介绍一下 RVM 在分类任务的应用。</p> <p>在分类任务中，我们还是保持模型 \(w\) 的高斯先验分布，同样为每个权参数 \(w_i\) 都引入一个单独的超参数控制精度。只是此时的输出变为 \(y(x) = sigmoid(w^T\phi(x))\) (在多分类问题中则变为 softmax)</p> <p>和回归问题不同的是，这里不再对模型 \(w\) 解析求积分来求得最大化边缘似然函数，而是使用了拉普拉斯近似的方法求解。</p> <p>最后总结下 RVM 的优劣</p> <ul> <li>RVM的相关向量的数量比SVM中使用的支持向量的数量少很多，对分类任务和回归任务，RVM生成的模型通常比对应的支持向量机生成的模型简洁一个数量级，使得处理测试数据的速度又极大提升。</li> <li>RVM的主要缺点是，相比较SVM，训练时间相对较长。但是RVM避免了通过交叉验证确定模型复杂度的过程，从而补偿了训练时间的劣势。此外它产生的模型更加稀疏，所以它对于测试点进行预测的计算时间通常更短，对于预测点的计算时间通常在实际应用中更加重要。</li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2023 Aofan Jiang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: November 14, 2023. </div> <div id="map-container" style="display: none;"> <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&amp;w=300&amp;t=n&amp;d=vcetvMKxQeU0A74GGVddvtKdDYpzY562Hjs3OBdOjBw"></script> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>