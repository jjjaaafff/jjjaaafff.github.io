---
layout: distill
title: Bounds and Sensitivity Analysis
date: 2021-07-16
description: Lecture Notes on Causal Inference
tags: CausalInference
related_posts: false


toc:
  - name: Bounds
  - name: Sensitivity Analysis

---

## Bounds
在之前的(T,W,Y)例子中，还可能存在潜在原因U同时作用与T和Y。此时，不存在未观察到的Confounding这样一种假设是不现实的。如果还是基于这种unconfoundedness假设，那么我们indentify的其实是一个point (一个具体的值)，而如果我们做一些更弱的假设，则此时我们indentify的是一个区间。在这里，我们来确定不同假设下该区间的范围，即为bounds

### No-Assumption Bound
在这里，我们假设所有的potential outcomes都具有统一的bound，即 $\forall t, a\le Y(t) \le b$  
由此我们可以得到 $a-b\le E[Y(1)-Y(0)] \le b-a$ ，整个区间的长度为 2(b-a) 。我们可以通过Observational-Counterfactual Decomposition来进一步压缩区间长度。首先我们用 $\pi$ 来表示 P(T=1)，根据定义展开ATE，可得到

$$E[Y(1)-Y(0)]=\pi E[Y\vert T=1]+(1-\pi) E[Y(1)\vert T=0] -\pi E[Y(0)\vert T=1]-(1-\pi) E[Y\vert T=0]$$

其中，展开后的第一项和第四项称为 observation term，可以从观察结果中直接计算得到。而第二项和第三项称为 Counterfactual term，无法通过计算得到。为此，我们保留observation term，将counterfactual term替换为(a,b) bound 区间。此时我们可以得到

$$E[Y(1)-Y(0)]\le \pi E[Y\vert T=1]+(1-\pi) b -\pi a-(1-\pi) E[Y\vert T=0]$$

$$E[Y(1)-Y(0)]\ge \pi E[Y\vert T=1]+(1-\pi) a -\pi b-(1-\pi) E[Y\vert T=0]$$

我们可以发现，此时 E[Y(1)-Y(0)] 的区间长度被压缩到了 b-a

### Monotone Treatment Bound
* Monotone Treatment Response (MTR)  
  这个假设是指，我们假设treatment永远有帮助，即 $\forall i,\ Y_i(1) \ge Y_i(0)$  

  基于此假设，我们可以得到 $E[Y(1)\vert T=0]\ge E[Y\vert T=0]$ ，再结合Observational-Counterfactual Decomposition 得到的式子，我们可以将区间的下限提升至0，即 $E[Y(1)-Y(0)]\ge 0$  

  同样，如果我们假设treatment永远帮倒忙，则是将区间的上限降至0

* Monotone Treatment Selection (MTS)  
  这个假设是指，假设treatment group的潜在结果永远比control group结果更好，即 $E[Y (1) \vert T = 1] \ge E[Y (1) \vert T = 0], \ E[Y (0) \vert T = 1] \ge E[Y (0) \vert  T = 0]$ ，再结合Observational-Counterfactual Decomposition 得到的式子，我们可以得到区间的上界，即 $E[Y(1)-Y(0)]\le E[Y\vert T=1]-E[Y\vert T=0]$ 

### Optimal Treatment Selection
* OST1  
  这个假设是指，我们假设每个个体永远接受最好的treatment，即 $T_i=1 \Rightarrow Y_i(1) \ge Y_i(0), \ T_i=0 \Rightarrow Y_i(0) > Y_i(1)$  

  基于此假设，我们可以得到 $E[Y(1)\vert T=0]\le E[Y\vert T=0], \ E[Y (0) \vert T = 1] \le E[Y  \vert  T = 1]$ ，再结合展开式子，我们可以得到

  $$E[Y(1)-Y(0)]\le \pi E[Y\vert T=1]-\pi a$$
  
  $$E[Y(1)-Y(0)]\ge (1-\pi)a - (1-\pi) E[Y\vert T=0]$$

* OST2  
  以上提到的假设，都使得ATE区间包含0，即无法indentify ATE的符号。为此我们结合OST1假设及其逆否命题，得到OST2假设，可确定区间内的值均为正或负。  
  
  在OST2假设中，我们可以得到 $E[Y (1) \vert T = 0] \le E[Y \vert T = 1]$ 。该关系的证明如下：  
  首先利用OST1中的假设，我们有 $E[Y (1) \vert T = 0] = E[Y (1) \vert Y_i(0)>Y_i(1)] \le E[Y (1) \vert Y_i(0)\le Y_i(1)]$  
  在结合OST1假设的逆否命题，我们有 $E[Y (1) \vert Y_i(0)\le Y_i(1)] = E[Y(1)\vert T=1]$  
  结合以上两个式子，即可得到 OST2 假设的内容， $E[Y (1) \vert T = 0] \le E[Y \vert T = 1]$

  在知道这个假设之后，我们结合展开式化简，可以得到对应的区间上下界，即

  $$E[Y(1)-Y(0)]\le E[Y\vert T=1]-\pi a - (1-\pi)E[Y\vert T=0]$$
  
  $$E[Y(1)-Y(0)]\ge \pi E[Y\vert T=1]+(1-\pi) a - E[Y\vert T=0]$$


总的来说，具体使用哪种假设取决于具体问题。在有的问题里，某些假设可能完全没有意义，而在其他问题里，又可能是最合适的假设 (区间范围最小等)。因此需要具体情况具体分析




## Sensitivity Analysis
正如之前所提到的 (T,W,U,Y) 例子，真实的causal effect应该是 $E_{W,U}[...]$ ，而因为我们无法观测到U，实际上我们计算的是 $E_{W}[...]$ ，这就导致了偏差的产生。sensitivity analysis 就是为了分析这样一种偏差程度

* Linear Single Confounder  
  我们主要考虑最简单的一种情况，即线性关系下的敏感性分析。  
  我们定义 $T:=\alpha_w W+\alpha_u U$ 和 $Y:=\beta_w W+\beta_u U + \delta T$ 。  
  在理想情况(全知全能)下，causal effect 就是 $\delta$ ，我们的目的也就是恢复 $\delta$ 。  
  而近似情况下，$E_W[...] = \delta + \frac{\beta_u}{\alpha_u}$  
  此时的偏置值 (bias) 为近似减真实，即 $E_W[...]-E_{W,U}[...]=\beta_u / \alpha_u$  
  对应的causal effect可视为 $\delta = Constant-\beta_u / \alpha_u$ ，我们也可以借此找到 causal effect>0 的分界线。根据 $\beta_u - 1/\alpha_u$ 坐标轴作图找分界线的方式也称为 contour plot，经常应用于敏感性分析。

  接下来是上述公式的证明
  1. 首先通过化简得到 $E_W [E[Y \vert T = t, W]]=(\delta+\frac{\beta_u}{\alpha_u})t+(\beta_w-\frac{\beta_u\alpha_w}{\alpha_u})E[W]$
  2. 分别代入t=1和t=0，并相减即可得到 $E_W [E[Y \vert T = 1, W] - E[Y \vert T = 0, W]] = \delta+\frac{\beta_u}{\alpha_u}$


以上是最简单形式的 Linear SCM，for arbitrary estimands in arbitrary graphs, where the structural equations are still linear，对应的参考文献为 [Sensitivity Analysis of Linear Structural Causal Models” from Cinelli et al. (2019)](http://proceedings.mlr.press/v97/cinelli19a.html)


其他更广泛形式下的敏感性分析方法见课后参考文档
